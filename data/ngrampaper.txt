

training time. The basic Skip-gram formulation definesp(wt+j|wt)using the softmax function:p(wO|wI) =exp(v′wO⊤vwI)∑Ww=1exp(v′w⊤vwI)(2)wherevwandv′ware the  “input” and “output” vector representations ofw,  andWis  the  num-ber  of  words  in  the  vocabulary.   This  formulation is  impractical because  the  cost  of  computing∇logp(wO|wI)is proportional toW, which is often large (105–107terms).2.1    Hierarchical SoftmaxA computationally efficient approximation of the full softmax is the hierarchical softmax.  In thecontext of neural network language models, it was first introduced by Morin and Bengio [12].  Themain advantage is that instead of evaluatingWoutput nodes in the neural network to obtain theprobability distribution, it is needed to evaluate only aboutlog2(W)nodes.The hierarchical softmax uses a binary tree representationof the output layer with theWwords asits leaves and, for each node, explicitly represents the relative probabilities of its child nodes. Thesedefine a random walk that assigns probabilities to words.More precisely, each wordwcan be reached by an appropriate path from the root of the tree.  Letn(w, j)be thej-th node on the path from the root tow, and letL(w)be the length of this path, son(w,1) = rootandn(w, L(w)) =w.  In addition, for any inner noden, letch(n)be an arbitraryfixed child ofnand let[[x]]be 1 ifxis true and -1 otherwise. Then the hierarchical softmax definesp(wO|wI)as follows:p(w|wI) =L(w)−1∏j=1σ([[n(w, j+ 1) = ch(n(w, j))]]·v′n(w,j)⊤vwI)(3)whereσ(x) = 1/(1 + exp(−x)). It can be verified that∑Ww=1p(w|wI) = 1. This implies that thecost of computinglogp(wO|wI)and∇logp(wO|wI)is proportional toL(wO), which on averageis no greater thanlogW.  Also, unlike the standard softmax formulation of the Skip-gram whichassigns two representationsvwandv′wto each wordw,  the hierarchical softmax formulation hasone representationvwfor each wordwand one representationv′nfor every inner nodenof thebinary tree.The structure of the tree used by the hierarchical softmax has a considerable effect on the perfor-mance.  Mnih and Hinton explored a number of methods for constructing the tree structure and theeffect on both the training time and the resulting model accuracy [10]. In our work we use a binaryHuffman tree, as it assigns short codes to the frequent wordswhich results in fast training.  It hasbeen observed before that grouping words together by their frequency works well as a very simplespeedup technique for the neural network based language models [5, 8].2.2    Negative SamplingAn alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-troduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].NCE posits that a good model should be able to differentiate data from noise by means of logisticregression.  This is similar to hinge loss used by Collobert and Weston [2] who trained the modelsby ranking the data above noise.While NCE can be shown to approximately maximize the log probability of the softmax, the Skip-gram model is only concerned with learning high-quality vector representations, so we are free tosimplify NCE as long as the vector representations retain their quality. We define Negative sampling(NEG) by the objectivelogσ(v′wO⊤vwI) +k∑i=1Ewi∼Pn(w)[logσ(−v′wi⊤vwI)](4)3
-2-1.5-1-0.5 0 0.5 1 1.5 2-2-1.5-1-0.5 0 0.5 1 1.5 2Country and Capital Vectors Projected by PCAChinaJapanFranceRussiaGermanyItalySpainGreeceTurkeyBeijingParisTokyoPolandMoscowPortugalBerlinRomeAthensMadridAnkaraWarsawLisbonFigure 2:Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and theircapital cities. The figure illustrates ability of the model to automatically organize concepts and learn implicitlythe relationships between them, as during the training we did not provide any supervised information aboutwhat a capital city means.which is used to replace everylogP(wO|wI)term in the Skip-gram objective.  Thus the task is todistinguish the target wordwOfrom draws from the noise distributionPn(w)using logistic regres-sion, where there areknegative samples for each data sample. Our experiments indicate that valuesofkin the range 5–20 are useful for small training datasets, while for large datasets thekcan be assmall as 2–5. The main difference between the Negative sampling and NCE is that NCE needs bothsamples and the numerical probabilities of the noise distribution, while Negative sampling uses onlysamples. And while NCE approximately maximizes the log probability of the softmax, this propertyis not important for our application.Both NCE and NEG have the noise distributionPn(w)as a free parameter. We investigated a numberof choices forPn(w)and found that the unigram distributionU(w)raised to the3/4rd power (i.e.,U(w)3/4/Z) outperformed significantly the unigram and the uniform distributions, for both NCEand NEG on every task we tried including language modeling (not reported here).2.3    Subsampling of Frequent WordsIn very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g.,“in”, “the”, and “a”).  Such words usually provide less information value than the rare words.  Forexample, while the Skip-gram model benefits from observing the co-occurrences of “France” and“Paris”, it benefits much less from observing the frequent co-occurrences of “France” and “the”, asnearly every word co-occurs frequently within a sentence with “the”. This idea can also be appliedin the opposite direction; the vector representations of frequent words do not change significantlyafter training on several million examples.To counter the imbalance between the rare and frequent words, we used a simple subsampling ap-proach: each wordwiin the training set is discarded with probability computed by the formulaP(wi) = 1−√tf(wi)(5)4
MethodTime [min]Syntactic [%]Semantic [%]Total accuracy [%]NEG-538635459NEG-1597635861HS-Huffman41534047NCE-538604553The following results use10−5subsamplingNEG-514615860NEG-1536616161HS-Huffman21525955Table 1:  Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning taskas defined in [8].  NEG-kstands for Negative Sampling withknegative samples for each positivesample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the HierarchicalSoftmax with the frequency-based Huffman codes.wheref(wi)is  the  frequency  of  wordwiandtis  a  chosen  threshold,  typically  around10−5.We  chose  this subsampling formula because  it  aggressively subsamples words whose  frequencyis greater thantwhile preserving the ranking of the frequencies.  Although this subsampling for-mula was chosen heuristically, we found it to work well in practice. It accelerates learning and evensignificantly improves the accuracy of the learned vectors of the rare words, as will be shown in thefollowing sections.3    Empirical ResultsIn this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, NegativeSampling, and subsampling of the training words. We used theanalogical reasoning task1introducedby Mikolov et al. [8]. The task consists of analogies such as “Germany” : “Berlin” :: “France” : ?,which are solved by finding a vectorxsuch that vec(x) is closest to vec(“Berlin”) - vec(“Germany”)+ vec(“France”) according to the cosine distance (we discard the input words from the search). This specific example is considered to have been answered correctly ifxis “Paris”.  The task has twobroad categories: the syntactic analogies (such as “quick”: “quickly” :: “slow” : “slowly”) and thesemantic analogies, such as the country to capital city relationship.For training the Skip-gram models, we have used a large dataset consisting of various news articles(an internal Google dataset with one billion words).  We discarded from the vocabulary all wordsthat occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K.The performance of various Skip-gram models on the word analogy test set is reported in Table 1.The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogicalreasoning task, and has even slightly better performance than the Noise Contrastive Estimation. Thesubsampling of the frequent words improves the training speed several times and makes the wordre presentations significantly more accurate.It can be argued that the linearity of the skip-gram model makes its vectors more suitable for suchlinear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learnedby the standard sigmoidal recurrent neural networks (whichare highly non-linear) improve on thistask significantly as the amount of the training data increases, suggesting that non-linear models alsohave a preference for a linear structure of the word representations.4    Learning PhrasesAs discussed earlier, many phrases have a meaning that is nota simple composition of the mean-ings of its individual words.   To learn vector representation for phrases,  we first find words thatappear frequently together, and infrequently in other contexts. For example, “New York Times” and“Toronto Maple Leafs” are replaced by unique tokens in the training data, while a bigram “this is”will remain unchanged.1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt5
NewspapersNew YorkNew York TimesBaltimoreBaltimore SunSan JoseSan Jose Mercury NewsCincinnatiCincinnati EnquirerNHL TeamsBostonBoston BruinsMontrealMontreal CanadiensPhoenixPhoenix CoyotesNashvilleNashville PredatorsNBA TeamsDetroitDetroit PistonsTorontoToronto RaptorsOaklandGolden State WarriorsMemphisMemphis GrizzliesAirlinesAustriaAustrian AirlinesSpainSpainairBelgiumBrussels AirlinesGreeceAegean AirlinesCompany executivesSteve BallmerMicrosoftLarry PageGoogleSamuel J. PalmisanoIBMWerner VogelsAmazonTable 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples).The goal is to compute the fourth phrase using the first three.Our best model achieved an accuracyof 72% on this dataset.This way, we can form many reasonable phrases without greatly increasing the size of the vocabu-lary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memoryintensive. Many techniques have been previously developedto identify phrases in the text; however,it is out of scope of our work to compare them.  We decided to usea simple data-driven approach,where phrases are formed based on the unigram and bigram counts, usingscore(wi, wj) =count(wiwj)−δcount(wi)×count(wj).(6)Theδis used as a discounting coefficient and prevents too many phrases consisting of very infre-quent words to be formed.  The bigrams with score above the chosen threshold are then used asphrases. Typically, we run 2-4 passes over the training datawith decreasing threshold value, allow-ing longer phrases that consists of several words to be formed. We evaluate the quality of the phraserepresentations using a new analogical reasoning task thatinvolves phrases. Table 2 shows examplesof the five categories of analogies used in this task. This dataset is publicly available on the web2.4.1    Phrase Skip-Gram ResultsStarting with the same news data as in the previous experiments, we first constructed the phrasebased  training  corpus  and  then  we  trained  several  Skip-grammodels  using  different  hyper-parameters.  As before, we used vector dimensionality 300 and context size 5.  This setting alreadyachieves good performance on the phrase dataset, and allowed us to quickly compare the NegativeSampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens.The results are summarized in Table 3.The results show that while Negative Sampling achieves a respectable accuracy even withk= 5,usingk= 15achieves considerably better performance.  Surprisingly,while we found the Hierar-chical Softmax to achieve lower performance when trained without subsampling, it became the bestperforming method when we downsampled the frequent words.  This shows that the subsamplingcan result in faster training and can also improve accuracy,at least in some cases.2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txtMethodDimensionalityNo subsampling [%]10−5subsampling [%]NEG-53002427NEG-153002742HS-Huffman3001947Table  3:  Accuracies of the  Skip-gram models on  the phrase analogy dataset.   The  models weretrained on approximately one billion words from the news dataset.6
